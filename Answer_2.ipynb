{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Below is the derivation for Question 1 under the assumptions:\n\n* embedding size = $m$\n* hidden size (encoder & decoder) = $k$\n* input and output sequence length = $T$\n* (source and target) vocabulary size = $V$\n* simple RNN cell (not LSTM/GRU), one layer each\n\n---\n\n### (a) Total number of computations (per example, forward pass)\n\n1. **Encoder RNN** (over $T$ time-steps)\n   At each step you compute\n\n   $$\n     h_t = \\phi\\bigl(W_x x_t + W_h h_{t-1} + b\\bigr)\n   $$\n\n   * $W_x x_t$:  $m\\times k$ matrix–vector multiply ⇒ $m\\,k$ multiplies + $m\\,k$ adds\n   * $W_h h_{t-1}$:  $k\\times k$ multiply ⇒ $k^2$ multiplies + $k^2$ adds\n   * elementwise nonlinearity: $\\sim k$ ops\n\n   **≈** $(2\\,m\\,k + 2\\,k^2 + k)$ flops per step\n   → over $T$ steps:\n\n   $$\n     \\underbrace{T\\,(2\\,m\\,k + 2\\,k^2 + k)}_{\\text{encoder}}\n   $$\n\n2. **Decoder RNN + output projection** (also $T$ steps)\n\n   * RNN cell (same cost as above): $(2\\,m\\,k + 2\\,k^2 + k)$ per step\n   * output layer $o_t = W_o\\,h_t + b_o$: $k\\times V$ mat-vec ⇒ $k\\,V$ multiplies + $k\\,V$ adds\n\n   **≈** $(2\\,m\\,k + 2\\,k^2 + k) + 2\\,k\\,V$ flops per step\n   → over $T$ steps:\n\n   $$\n     \\underbrace{T\\,(2\\,m\\,k + 2\\,k^2 + k + 2\\,k\\,V)}_{\\text{decoder + projection}}\n   $$\n\n3. **Total**\n\n$$\n  \\boxed{\n    \\underbrace{T\\,(2\\,m\\,k + 2\\,k^2 + k)}_{\\text{encoder}}\n    \\;+\\;\n    \\underbrace{T\\,(2\\,m\\,k + 2\\,k^2 + k + 2\\,k\\,V)}_{\\text{decoder}}\n    \\;=\\;\n    T\\bigl(4\\,m\\,k \\;+\\;4\\,k^2 \\;+\\;2\\,k \\;+\\;2\\,k\\,V\\bigr)\n  }\n$$\n\nIf you count only the multiplies (ignoring adds and activation overhead), that simplifies to\n\n$$\n  T\\,(2\\,m\\,k + 2\\,k^2) + T\\,(2\\,m\\,k + 2\\,k^2 + k\\,V)\n  \\;=\\;\n  T\\bigl(4\\,m\\,k +4\\,k^2 + k\\,V\\bigr).\n$$\n\n---\n\n### (b) Total number of parameters\n\n1. **Input embedding**\n\n$$\n  \\mathbf{E}\\in\\mathbb R^{V \\times m}\n  \\quad\\Longrightarrow\\quad\n  V\\,m\n$$\n\n2. **Encoder RNN (one layer)**\n\n   * $W_x\\in\\mathbb R^{m\\times k}$: $m\\,k$\n   * $W_h\\in\\mathbb R^{k\\times k}$: $k^2$\n   * bias $b\\in\\mathbb R^k$: $k$\n     **⇒** $m\\,k + k^2 + k$\n\n3. **Decoder RNN (one layer)**\n   (same dimensions $m\\!\\to\\!k$ if you feed back embeddings, or $k\\!\\to\\!k$ if you feed last hidden as “input” — we’ll assume same as encoder)\n   **⇒** $m\\,k + k^2 + k$\n\n4. **Output layer**\n\n   * $W_o\\in\\mathbb R^{k\\times V}$: $k\\,V$\n   * bias $b_o\\in\\mathbb R^V$: $V$\n     **⇒** $k\\,V + V$\n\n---\n\nPutting it all together:\n\n$$\n\\boxed{\n  \\underbrace{V\\,m}_{\\text{embedding}}\n  \\;+\\;\n  \\underbrace{(m\\,k + k^2 + k)}_{\\text{encoder}}\n  \\;+\\;\n  \\underbrace{(m\\,k + k^2 + k)}_{\\text{decoder}}\n  \\;+\\;\n  \\underbrace{(k\\,V + V)}_{\\text{output}}\n  \\;=\\;\n  V\\,m \\;+\\; 2\\,m\\,k \\;+\\; 2\\,k^2 \\;+\\; 2\\,k \\;+\\; k\\,V \\;+\\; V\n}\n$$\n\nOmitting biases for brevity, the leading terms are\n\n$$\n  \\boxed{V\\,m \\;+\\;2\\,m\\,k \\;+\\;2\\,k^2 \\;+\\;k\\,V}.\n$$\n\n---\n\n**Summary**\n\n* **(a)** $\\displaystyle\\mathcal O\\bigl(T\\,(4\\,m\\,k +4\\,k^2 +2\\,k\\,V)\\bigr)$ flops\n* **(b)** $\\displaystyle V\\,m +2\\,m\\,k +2\\,k^2 +k\\,V$ (plus lower-order biases) parameters.\n","metadata":{}},{"cell_type":"code","source":"import wandb\nimport torch\nimport torch.nn as nn\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ANSWER 1","metadata":{}},{"cell_type":"code","source":"\n\nclass Encoder(nn.Module):\n    def __init__(self,\n                 src_vocab_size:int,\n                 embed_size:int,\n                 hidden_size:int,\n                 num_layers:int = 1,\n                 cell_type:str = \"RNN\",\n                 dropout:float = 0.0):\n        super().__init__()\n        self.embedding = nn.Embedding(src_vocab_size, embed_size)\n        # choose RNN/LSTM/GRU\n        rnn_cls = {\"RNN\": nn.RNN, \"LSTM\": nn.LSTM, \"GRU\": nn.GRU}[cell_type]\n        self.rnn = rnn_cls(\n            input_size=embed_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers>1 else 0.0\n        )\n        \n    def forward(self, src):\n        # src: (batch, src_len)\n        emb = self.embedding(src)               # (batch, src_len, embed_size)\n        outputs, hidden = self.rnn(emb)         # outputs: (batch, src_len, hidden)\n        # hidden: \n        #   RNN/GRU -> (num_layers, batch, hidden)\n        #   LSTM    -> tuple of two such tensors (h_n, c_n)\n        return outputs, hidden\n\nclass Decoder(nn.Module):\n    def __init__(self,\n                 trg_vocab_size:int,\n                 embed_size:int,\n                 hidden_size:int,\n                 num_layers:int = 1,\n                 cell_type:str = \"RNN\",\n                 dropout:float = 0.0):\n        super().__init__()\n        self.embedding = nn.Embedding(trg_vocab_size, embed_size)\n        rnn_cls = {\"RNN\": nn.RNN, \"LSTM\": nn.LSTM, \"GRU\": nn.GRU}[cell_type]\n        self.rnn = rnn_cls(\n            input_size=embed_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers>1 else 0.0\n        )\n        self.fc_out = nn.Linear(hidden_size, trg_vocab_size)\n        \n    def forward(self, input_token, hidden):\n        # input_token: (batch,)  — a single timestep token\n        input_token = input_token.unsqueeze(1)   # (batch, 1)\n        emb = self.embedding(input_token)        # (batch, 1, embed_size)\n        output, hidden = self.rnn(emb, hidden)   # output: (batch, 1, hidden)\n        prediction = self.fc_out(output.squeeze(1))  \n        # prediction: (batch, trg_vocab_size)\n        return prediction, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self,\n                 encoder:Encoder,\n                 decoder:Decoder,\n                 device:torch.device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio:float = 0.5):\n        batch_size, trg_len = trg.shape\n        trg_vocab_size = self.decoder.embedding.num_embeddings\n        \n        # tensor to store decoder outputs\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n\n        # 1. Encode the whole source sequence\n        _, hidden = self.encoder(src)\n        \n        # 2. first input to the decoder is the <sos> tokens\n        input_token = trg[:,0]  \n\n        for t in range(1, trg_len):\n            # 3. decode one token\n            pred, hidden = self.decoder(input_token, hidden)\n            outputs[:, t] = pred\n            \n            # 4. decide if we do teacher forcing\n            teacher_force = (torch.rand(1).item() < teacher_forcing_ratio)\n            top1 = pred.argmax(1)  # (batch,)\n            \n            input_token = trg[:, t] if teacher_force else top1\n\n        return outputs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T16:24:26.029987Z","iopub.execute_input":"2025-05-19T16:24:26.030585Z","iopub.status.idle":"2025-05-19T16:24:31.413795Z","shell.execute_reply.started":"2025-05-19T16:24:26.030533Z","shell.execute_reply":"2025-05-19T16:24:31.412687Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# SRC_VOCAB = len(src_token_to_idx)   # e.g. romanized chars\n# TRG_VOCAB = len(trg_token_to_idx)   # e.g. Devanagari chars\n# EMB_SIZE  = 64\n# HID_SIZE  = 128\n# LAYERS    = 2\n# CELL      = \"LSTM\"   # or \"RNN\", \"GRU\"\n# DROPOUT   = 0.3\n\n# enc = Encoder(SRC_VOCAB, EMB_SIZE, HID_SIZE, LAYERS, CELL, DROPOUT)\n# dec = Decoder(TRG_VOCAB, EMB_SIZE, HID_SIZE, LAYERS, CELL, DROPOUT)\n# model = Seq2Seq(enc, dec, device).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T16:45:07.966993Z","iopub.execute_input":"2025-05-19T16:45:07.967425Z","iopub.status.idle":"2025-05-19T16:45:07.972170Z","shell.execute_reply.started":"2025-05-19T16:45:07.967394Z","shell.execute_reply":"2025-05-19T16:45:07.971186Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"wandb.login(key=\"f0880f1a8675dc5a9ff218689c5340669690b6e0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T16:49:51.573219Z","iopub.execute_input":"2025-05-19T16:49:51.573574Z","iopub.status.idle":"2025-05-19T16:49:51.881690Z","shell.execute_reply.started":"2025-05-19T16:49:51.573550Z","shell.execute_reply":"2025-05-19T16:49:51.880794Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33med24s401\u001b[0m (\u001b[33med24s401-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"### ANSWER 2","metadata":{}},{"cell_type":"code","source":"# train.py\nimport wandb\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom your_seq2seq_module import Encoder, Decoder, Seq2Seq  # import from your code\nfrom dakshina_loader import DakshinaDataset              # or however you load hi lexicons\n\ndef train():\n    # 1️⃣ Initialize a new W&B run\n    wandb.init()\n    config = wandb.config   # holds all sweep parameters\n\n    # 2️⃣ Prepare data\n    train_ds = DakshinaDataset(lang=\"hi\", split=\"train\")\n    val_ds   = DakshinaDataset(lang=\"hi\", split=\"dev\")\n    train_dl = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True)\n    val_dl   = DataLoader(val_ds,   batch_size=config.batch_size)\n\n    # 3️⃣ Build model with hyperparameters from config\n    enc = Encoder(\n        src_vocab_size = train_ds.src_vocab_size,\n        embed_size     = config.embed_size,\n        hidden_size    = config.hidden_size,\n        num_layers     = config.enc_layers,\n        cell_type      = config.cell_type,\n        dropout        = config.dropout,\n    )\n    dec = Decoder(\n        trg_vocab_size = train_ds.trg_vocab_size,\n        embed_size     = config.embed_size,\n        hidden_size    = config.hidden_size,\n        num_layers     = config.dec_layers,\n        cell_type      = config.cell_type,\n        dropout        = config.dropout,\n    )\n    model = Seq2Seq(enc, dec, device=wandb.config.device).to(wandb.config.device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    criterion = nn.CrossEntropyLoss(ignore_index=train_ds.pad_idx)\n\n    # 4️⃣ Training loop\n    for epoch in range(config.epochs):\n        model.train()\n        total_loss = 0\n        correct, total = 0, 0\n\n        for src, trg in train_dl:\n            src, trg = src.to(wandb.config.device), trg.to(wandb.config.device)\n            optimizer.zero_grad()\n            outputs = model(src, trg, teacher_forcing_ratio=0.5)\n            # outputs: (B, T, V) -> reshape for CE\n            B, T, V = outputs.shape\n            loss = criterion(outputs[:,1:,:].reshape(-1, V),\n                             trg[:,1:].reshape(-1))\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        # 5️⃣ Validation\n        model.eval()\n        val_loss, val_correct, val_total = 0, 0, 0\n        with torch.no_grad():\n            for src, trg in val_dl:\n                src, trg = src.to(wandb.config.device), trg.to(wandb.config.device)\n                outputs = model(src, trg, teacher_forcing_ratio=0.0)\n                B, T, V = outputs.shape\n                loss = criterion(outputs[:,1:,:].reshape(-1, V),\n                                 trg[:,1:].reshape(-1))\n                val_loss += loss.item()\n\n                # compute simple accuracy over non-pad tokens\n                preds = outputs.argmax(-1)\n                mask = trg[:,1:] != train_ds.pad_idx\n                val_correct += (preds[:,1:][mask] == trg[:,1:][mask]).sum().item()\n                val_total   += mask.sum().item()\n\n        val_acc = val_correct / val_total\n\n        # 6️⃣ Log to W&B\n        wandb.log({\n            \"train_loss\": total_loss/len(train_dl),\n            \"val_loss\":   val_loss/len(val_dl),\n            \"val_accuracy\": val_acc,\n            \"epoch\": epoch\n        })\n\n    # 7️⃣ (Optional) Save best model/artifacts\n    torch.save(model.state_dict(), \"model.pt\")\n    wandb.save(\"model.pt\")\n\n\nif __name__ == \"__main__\":\n    train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}