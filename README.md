# ed24s401_dl_Assignment_3
# Seq2Seq Transliteration: Roman ‚Üî Devanagari

This repository implements a character‚Äêlevel sequence-to-sequence system for transliterating between Romanized text and Devanagari script.  It includes both a vanilla (no-attention) model and an additive-attention model, along with training, evaluation, and visualization scripts.

---

## üå≥ Repository Structure

```
Part_A/
‚îú‚îÄ‚îÄ lexicons/  
‚îÇ   ‚îú‚îÄ‚îÄ hi.translit.sampled.train.tsv  
‚îÇ   ‚îú‚îÄ‚îÄ hi.translit.sampled.dev.tsv  
‚îÇ   ‚îî‚îÄ‚îÄ hi.translit.sampled.test.tsv  
‚îÇ
‚îú‚îÄ‚îÄ models/  
‚îÇ   ‚îú‚îÄ‚îÄ best_model_vanilla.pt  
‚îÇ   ‚îî‚îÄ‚îÄ best_attention_model2.pt  
‚îÇ
‚îú‚îÄ‚îÄ predictions_vanilla/  
‚îÇ   ‚îî‚îÄ‚îÄ test_predictions.tsv  
‚îÇ
‚îú‚îÄ‚îÄ predictions_attention/  
‚îÇ   ‚îî‚îÄ‚îÄ test_predictions_attention.tsv  
‚îÇ
‚îú‚îÄ‚îÄ q5_heatmaps/  
‚îÇ   ‚îî‚îÄ‚îÄ attention_grid_3x3-4.png  
‚îÇ
‚îú‚îÄ‚îÄ q6_visualisation/  
‚îÇ   ‚îî‚îÄ‚îÄ attention_connectivity_grid.png  
‚îÇ
‚îú‚îÄ‚îÄ utils.py                   # Data loaders, vocab builders, collate function  
‚îú‚îÄ‚îÄ train_vanilla.py           # Train vanilla Seq2Seq with fixed hyperparameters  
‚îú‚îÄ‚îÄ sweep_vanilla.py           # WandB hyperparameter sweep for vanilla model  
‚îú‚îÄ‚îÄ infer_vanilla.py           # Generate vanilla model predictions on test set  
‚îú‚îÄ‚îÄ q4_visualize.py            # Display a grid of sample vanilla predictions  
‚îú‚îÄ‚îÄ compare_corrections.py     # Show cases where attention fixes vanilla errors  
‚îú‚îÄ‚îÄ train_attention.py         # Train attention-augmented Seq2Seq with fixed hyperparameters  
‚îú‚îÄ‚îÄ sweep_attention.py         # WandB hyperparameter sweep for attention model  
‚îú‚îÄ‚îÄ infer_attention.py         # Generate attention model predictions on test set  
‚îú‚îÄ‚îÄ q6_connectivity.py         # ‚ÄúConnectivity‚Äù gradient-norm visualization  
‚îî‚îÄ‚îÄ requirements.txt           # Python dependencies
README.md                       # This file
```

All scripts assume that `Part_A/lexicons` sits alongside them, containing the three `.tsv` files.

---

## ‚öôÔ∏è Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/your-username/ed24s401_dl_Assignment_3.git
   cd ed24s401_dl_Assignment_3/Part_A
   ```

2. **Create a virtual environment & install dependencies**

   ```bash
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

3. **Install Devanagari fonts (Ubuntu/Debian)**

   ```bash
   sudo apt-get update
   sudo apt-get install -y fonts-noto-core fonts-noto-dev fonts-noto-indic
   fc-cache -f -v
   ```

   In Python scripts, ensure you set:

   ```python
   import matplotlib as mpl
   mpl.rcParams['font.family'] = ['Noto Sans Devanagari','DejaVu Sans']
   ```

---

## üöÄ Usage

### Training

* **Vanilla model (fixed hyperparameters)**

  ```bash
  python train_vanilla.py \
    --emb_dim 256 --hid_dim 256 --enc_layers 3 --dec_layers 2 \
    --cell LSTM --dropout 0.2 --lr 1e-3 --batch_size 128
  ```

  Produces `models/best_model_vanilla.pt`.

* **Attention model (fixed hyperparameters)**

  ```bash
  python train_attention.py \
    --emb_dim 64 --hid_dim 256 --enc_layers 3 --dec_layers 1 \
    --cell LSTM --dropout 0.3 --lr 1e-3 --batch_size 128
  ```

  Produces `models/best_attention_model2.pt`.

### Sweeps (optional)

* **Vanilla sweep**

  ```bash
  python sweep_vanilla.py
  ```
* **Attention sweep**

  ```bash
  python sweep_attention.py
  ```

  View and compare runs in your WandB project dashboard.

### Inference

* **Vanilla predictions**

  ```bash
  python infer_vanilla.py
  ```

  Writes `predictions_vanilla/test_predictions.tsv`.

* **Attention predictions**

  ```bash
  python infer_attention.py
  ```

  Writes `predictions_attention/test_predictions_attention.tsv`.

### Visualization

* **Sample grid of vanilla outputs**

  ```bash
  python q4_visualize.py
  ```

  Opens an HTML grid or inline display of 20 random test examples.

* **Attention heatmaps**
  Inspect `q5_heatmaps/attention_grid_3x3-4.png` (generated by `train_attention.py`).

* **Connectivity plots**

  ```bash
  python q6_connectivity.py
  ```

  Saves `q6_visualisation/attention_connectivity_grid.png`.

* **Error corrections**

  ```bash
  python compare_corrections.py
  ```

  Prints and/or saves cases where the attention model corrected vanilla errors.

---

## üìÑ Dependencies

```ini
# Core deep‚Äêlearning
torch>=1.8.0
torchvision>=0.9.0
torchaudio>=0.8.0

# Data processing
pandas>=1.1.0
numpy>=1.19.0

# Experiment tracking
wandb>=0.12.0

# Plotting & visualization
matplotlib>=3.3.0
seaborn>=0.11.0

# Utilities
tqdm>=4.56.0
```

---

## üîß Hyperparameters

### Vanilla (fixed)

| Parameter      | Value  |
| -------------- | ------ |
| RNN cell       | LSTM   |
| Embedding dim  | 256    |
| Hidden dim     | 256    |
| Encoder layers | 3      |
| Decoder layers | 2      |
| Dropout        | 0.20   |
| Learning rate  | 1√ó10‚Åª¬≥ |
| Batch size     | 128    |
| Beam size      | 1      |

### Attention (fixed)

| Parameter      | Value  |
| -------------- | ------ |
| RNN cell       | LSTM   |
| Embedding dim  | 64     |
| Hidden dim     | 256    |
| Encoder layers | 3      |
| Decoder layers | 1      |
| Dropout        | 0.30   |
| Learning rate  | 1√ó10‚Åª¬≥ |
| Batch size     | 128    |

---

